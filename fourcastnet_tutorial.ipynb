{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960750a3-457b-4e1e-ad53-c9ff3dc98793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a bare bones running of fourcastnet inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafebbd-998e-43b6-8325-c94c293a5032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f213e821-606b-434a-9d4d-7240f9c38ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path : /apps/cent7/xalt/site\n",
      "path : /apps/cent7/xalt/libexec\n",
      "path : /home/gupt1075/.conda/envs/cent7/2020.11-py38/chirain_pytorch/lib/python311.zip\n",
      "path : /home/gupt1075/.conda/envs/cent7/2020.11-py38/chirain_pytorch/lib/python3.11\n",
      "path : /home/gupt1075/.conda/envs/cent7/2020.11-py38/chirain_pytorch/lib/python3.11/lib-dynload\n",
      "path : \n",
      "path : /home/gupt1075/.conda/envs/cent7/2020.11-py38/chirain_pytorch/lib/python3.11/site-packages\n",
      "path : /scratch/gilbreth/gupt1075/FourCastNet/\n",
      "path : /scratch/gilbreth/gupt1075/FourCastNet/\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ruamel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mafnonet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AFNONet\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader_multifiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data_loader\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mYParams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YParams\n\u001b[1;32m     50\u001b[0m _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m log_level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO\n",
      "File \u001b[0;32m~/FourCastNet/utils/YParams.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mruamel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myaml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YAML\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mYParams\u001b[39;00m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ruamel'"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# sys.path.append(os.path.dirname(os.path.realpath(__file__)) + '/../')\n",
    "username = \"gupt1075\"\n",
    "#  added for reading custom earth2mip codebase\n",
    "sys.path.append(f\"/scratch/gilbreth/{username}/FourCastNet/\")\n",
    "# sys.path.remove(f\"~/FourCastNet/\")\n",
    "\n",
    "\n",
    "# Print all paths\n",
    "for path in sys.path:\n",
    "    print(f\"path : {path}\")\n",
    "\n",
    "\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from numpy.core.numeric import False_\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torchvision.utils import save_image\n",
    "from utils import logging_utils\n",
    "from utils.weighted_acc_rmse import (\n",
    "    unweighted_acc_torch_channels,\n",
    "    weighted_acc_masked_torch_channels,\n",
    "    weighted_acc_torch_channels,\n",
    "    weighted_rmse_torch_channels,\n",
    ")\n",
    "\n",
    "logging_utils.config_logger()\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import wandb\n",
    "from networks.afnonet import AFNONet\n",
    "from utils.data_loader_multifiles import get_data_loader\n",
    "from utils.YParams import YParams\n",
    "\n",
    "_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "log_level=logging.INFO\n",
    "\n",
    "\n",
    "\n",
    "#  /scratch/gilbreth/wwtung/FourCastNet\n",
    "logging.basicConfig(filename =f\"/scratch/gilbreth/{username}/FourCastNet/logs/fourcastnet_inference_default.err\", level=log_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79274d-e047-4fbf-a42c-4b6b12d163c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da27f9-3c8d-4830-be36-e9cf19c8ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_perturb(x, level=0.01, device=0):\n",
    "    noise = level * torch.randn(x.shape).to(device, dtype=torch.float)\n",
    "    return x + noise\n",
    "\n",
    "\n",
    "def load_model(model, params, checkpoint_file):\n",
    "    model.zero_grad()\n",
    "    checkpoint_fname = checkpoint_file\n",
    "    checkpoint = torch.load(checkpoint_fname)\n",
    "    try:\n",
    "        new_state_dict = OrderedDict()\n",
    "        for (key, val) in checkpoint['model_state'].items():\n",
    "            name = key[7:]\n",
    "            if name != 'ged':\n",
    "                new_state_dict[name] = val\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    except:\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def downsample(x, scale=0.125):\n",
    "    return torch.nn.functional.interpolate(x, scale_factor=scale, mode='bilinear')\n",
    "\n",
    "\n",
    "def setup(params):\n",
    "    device = (torch.cuda.current_device() if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # get data loader\n",
    "\n",
    "    (valid_data_loader, valid_dataset) = get_data_loader(params, params.inf_data_path, dist.is_initialized(), train=False)\n",
    "    img_shape_x = valid_dataset.img_shape_x\n",
    "    img_shape_y = valid_dataset.img_shape_y\n",
    "    params.img_shape_x = img_shape_x\n",
    "    params.img_shape_y = img_shape_y\n",
    "    if params.log_to_screen:\n",
    "        logging.info('Loading trained model checkpoint from {}'.format(params['best_checkpoint_path'\n",
    "                     ]))\n",
    "\n",
    "    in_channels = np.array(params.in_channels)\n",
    "    out_channels = np.array(params.out_channels)\n",
    "    n_in_channels = len(in_channels)\n",
    "    n_out_channels = len(out_channels)\n",
    "\n",
    "    if params['orography']:\n",
    "        params['N_in_channels'] = n_in_channels + 1\n",
    "    else:\n",
    "        params['N_in_channels'] = n_in_channels\n",
    "    params['N_out_channels'] = n_out_channels\n",
    "    params.means = np.load(params.global_means_path)[0, out_channels]  # needed to standardize wind data\n",
    "    params.stds = np.load(params.global_stds_path)[0, out_channels]\n",
    "\n",
    "    # load the model\n",
    "\n",
    "    if params.nettype == 'afno':\n",
    "        model = AFNONet(params).to(device)\n",
    "    else:\n",
    "        raise Exception('not implemented')\n",
    "\n",
    "    checkpoint_file = params['best_checkpoint_path']\n",
    "    model = load_model(model, params, checkpoint_file)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # load the validation data\n",
    "\n",
    "    files_paths = glob.glob(params.inf_data_path + '/*.h5')\n",
    "    logging.warning(f\" loading validation data {files_paths} \")\n",
    "    files_paths.sort()\n",
    "\n",
    "    # which year\n",
    "\n",
    "    yr = 0\n",
    "    if params.log_to_screen:\n",
    "        logging.info('Loading inference data')\n",
    "        logging.info('Inference data from {}'.format(files_paths[yr]))\n",
    "\n",
    "    valid_data_full = h5py.File(files_paths[yr], 'r')['fields']\n",
    "\n",
    "    return (valid_data_full, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109a808-4ed4-4814-913d-2dab3cf42068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a5d14-6058-45ce-b7d4-2d3447eefdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autoregressive_inference(\n",
    "    params,\n",
    "    ic,\n",
    "    valid_data_full,\n",
    "    model,\n",
    "    ):\n",
    "    ic = int(ic)\n",
    "\n",
    "    # initialize global variables\n",
    "\n",
    "    device = (torch.cuda.current_device() if torch.cuda.is_available() else 'cpu')\n",
    "    exp_dir = params['experiment_dir']\n",
    "    dt = int(params.dt)\n",
    "    prediction_length = int(params.prediction_length / dt)\n",
    "    n_history = params.n_history\n",
    "    img_shape_x = params.img_shape_x\n",
    "    img_shape_y = params.img_shape_y\n",
    "    in_channels = np.array(params.in_channels)\n",
    "    out_channels = np.array(params.out_channels)\n",
    "    n_in_channels = len(in_channels)\n",
    "    n_out_channels = len(out_channels)\n",
    "    means = params.means\n",
    "    stds = params.stds\n",
    "\n",
    "    # initialize memory for image sequences and RMSE/ACC\n",
    "\n",
    "    valid_loss = torch.zeros((prediction_length,\n",
    "                             n_out_channels)).to(device,\n",
    "            dtype=torch.float)\n",
    "    acc = torch.zeros((prediction_length, n_out_channels)).to(device,\n",
    "            dtype=torch.float)\n",
    "\n",
    "    # compute metrics in a coarse resolution too if params.interp is nonzero\n",
    "\n",
    "    valid_loss_coarse = torch.zeros((prediction_length,\n",
    "                                    n_out_channels)).to(device,\n",
    "            dtype=torch.float)\n",
    "    acc_coarse = torch.zeros((prediction_length,\n",
    "                             n_out_channels)).to(device,\n",
    "            dtype=torch.float)\n",
    "    acc_coarse_unweighted = torch.zeros((prediction_length,\n",
    "            n_out_channels)).to(device, dtype=torch.float)\n",
    "\n",
    "    acc_unweighted = torch.zeros((prediction_length,\n",
    "                                 n_out_channels)).to(device,\n",
    "            dtype=torch.float)\n",
    "    seq_real = torch.zeros((prediction_length, n_in_channels,\n",
    "                           img_shape_x, img_shape_y)).to(device,\n",
    "            dtype=torch.float)\n",
    "    seq_pred = torch.zeros((prediction_length, n_in_channels,\n",
    "                           img_shape_x, img_shape_y)).to(device,\n",
    "            dtype=torch.float)\n",
    "\n",
    "    acc_land = torch.zeros((prediction_length,\n",
    "                           n_out_channels)).to(device,\n",
    "            dtype=torch.float)\n",
    "    acc_sea = torch.zeros((prediction_length,\n",
    "                          n_out_channels)).to(device, dtype=torch.float)\n",
    "    if params.masked_acc:\n",
    "        maskarray = \\\n",
    "            torch.as_tensor(np.load(params.maskpath)[0:720]).to(device,\n",
    "                dtype=torch.float)\n",
    "\n",
    "    valid_data = valid_data_full[ic:ic + prediction_length * dt\n",
    "                                 + n_history * dt:dt, in_channels, 0:\n",
    "                                 720]  # extract valid data from first year\n",
    "\n",
    "    # standardize\n",
    "\n",
    "    valid_data = (valid_data - means) / stds\n",
    "    valid_data = torch.as_tensor(valid_data).to(device,\n",
    "            dtype=torch.float)\n",
    "\n",
    "    # load time means\n",
    "\n",
    "    if not params.use_daily_climatology:\n",
    "        m = \\\n",
    "            torch.as_tensor((np.load(params.time_means_path)[0][out_channels]\n",
    "                            - means) / stds)[:, 0:img_shape_x]  # climatology\n",
    "        m = torch.unsqueeze(m, 0)\n",
    "    else:\n",
    "\n",
    "      # use daily clim like weyn et al. (different from rasp)\n",
    "\n",
    "        dc_path = params.dc_path\n",
    "        with h5py.File(dc_path, 'r') as f:\n",
    "            dc = f['time_means_daily'][ic:ic + prediction_length * dt:\n",
    "                    dt]  # 1460,21,721,1440\n",
    "        m = torch.as_tensor((dc[:, out_channels, 0:img_shape_x, :]\n",
    "                            - means) / stds)\n",
    "\n",
    "    m = m.to(device, dtype=torch.float)\n",
    "    if params.interp > 0:\n",
    "        m_coarse = downsample(m, scale=params.interp)\n",
    "\n",
    "    std = torch.as_tensor(stds[:, 0, 0]).to(device, dtype=torch.float)\n",
    "\n",
    "    orography = params.orography\n",
    "    orography_path = params.orography_path\n",
    "    if orography:\n",
    "        orog = \\\n",
    "            torch.as_tensor(np.expand_dims(np.expand_dims((h5py.File(orography_path,\n",
    "                            'r')['orog'])[0:720], axis=0),\n",
    "                            axis=0)).to(device, dtype=torch.float)\n",
    "        logging.info('orography loaded; shape:{}'.format(orog.shape))\n",
    "\n",
    "    # autoregressive inference\n",
    "\n",
    "    if params.log_to_screen:\n",
    "        logging.info('Begin autoregressive inference')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(valid_data.shape[0]):\n",
    "            if i == 0:  # start of sequence\n",
    "                first = valid_data[0:n_history + 1]\n",
    "                future = valid_data[n_history + 1]\n",
    "                for h in range(n_history + 1):\n",
    "                    seq_real[h] = first[h * n_in_channels:(h + 1)\n",
    "                        * n_in_channels][0:n_out_channels]  # extract history from 1st\n",
    "                    seq_pred[h] = seq_real[h]\n",
    "                if params.perturb:\n",
    "                    first = gaussian_perturb(first,\n",
    "                            level=params.n_level, device=device)  # perturb the ic\n",
    "                if orography:\n",
    "                    future_pred = model(torch.cat((first, orog),\n",
    "                            axis=1))\n",
    "                else:\n",
    "                    future_pred = model(first)\n",
    "            else:\n",
    "                if i < prediction_length - 1:\n",
    "                    future = valid_data[n_history + i + 1]\n",
    "                if orography:\n",
    "                    future_pred = model(torch.cat((future_pred, orog),\n",
    "                            axis=1))  # autoregressive step\n",
    "                else:\n",
    "                    future_pred = model(future_pred)  # autoregressive step\n",
    "\n",
    "            if i < prediction_length - 1:  # not on the last step\n",
    "                seq_pred[n_history + i + 1] = future_pred\n",
    "                seq_real[n_history + i + 1] = future\n",
    "                history_stack = seq_pred[i + 1:i + 2 + n_history]\n",
    "\n",
    "            future_pred = history_stack\n",
    "\n",
    "        # Compute metrics\n",
    "\n",
    "            if params.use_daily_climatology:\n",
    "                clim = m[i:i + 1]\n",
    "                if params.interp > 0:\n",
    "                    clim_coarse = m_coarse[i:i + 1]\n",
    "            else:\n",
    "                clim = m\n",
    "                if params.interp > 0:\n",
    "                    clim_coarse = m_coarse\n",
    "\n",
    "            pred = torch.unsqueeze(seq_pred[i], 0)\n",
    "            tar = torch.unsqueeze(seq_real[i], 0)\n",
    "            valid_loss[i] = weighted_rmse_torch_channels(pred, tar) \\\n",
    "                * std\n",
    "            acc[i] = weighted_acc_torch_channels(pred - clim, tar\n",
    "                    - clim)\n",
    "            acc_unweighted[i] = unweighted_acc_torch_channels(pred\n",
    "                    - clim, tar - clim)\n",
    "\n",
    "            if params.masked_acc:\n",
    "                acc_land[i] = weighted_acc_masked_torch_channels(pred\n",
    "                        - clim, tar - clim, maskarray)\n",
    "                acc_sea[i] = weighted_acc_masked_torch_channels(pred\n",
    "                        - clim, tar - clim, 1 - maskarray)\n",
    "\n",
    "            if params.interp > 0:\n",
    "                pred = downsample(pred, scale=params.interp)\n",
    "                tar = downsample(tar, scale=params.interp)\n",
    "                valid_loss_coarse[i] = \\\n",
    "                    weighted_rmse_torch_channels(pred, tar) * std\n",
    "                acc_coarse[i] = weighted_acc_torch_channels(pred\n",
    "                        - clim_coarse, tar - clim_coarse)\n",
    "                acc_coarse_unweighted[i] = \\\n",
    "                    unweighted_acc_torch_channels(pred - clim_coarse,\n",
    "                        tar - clim_coarse)\n",
    "\n",
    "            if params.log_to_screen:\n",
    "                tmp_dict = params[\"idxes\"]\n",
    "                idx = tmp_dict[params[\"fld\"]]\n",
    "                logging.info('Predicted timestep {} of {}. {} RMS Error: {}, ACC: {}'.format(i,\n",
    "                             prediction_length, args.fld, valid_loss[i,\n",
    "                             idx], acc[i, idx]))\n",
    "                if params.interp > 0:\n",
    "                    logging.info('[COARSE] Predicted timestep {} of {}. {} RMS Error: {}, ACC: {}'.format(i,\n",
    "                                 prediction_length, args.fld,\n",
    "                                 valid_loss_coarse[i, idx],\n",
    "                                 acc_coarse[i, idx]))\n",
    "\n",
    "    seq_real = seq_real.cpu().numpy()\n",
    "    seq_pred = seq_pred.cpu().numpy()\n",
    "    valid_loss = valid_loss.cpu().numpy()\n",
    "    acc = acc.cpu().numpy()\n",
    "    acc_unweighted = acc_unweighted.cpu().numpy()\n",
    "    acc_coarse = acc_coarse.cpu().numpy()\n",
    "    acc_coarse_unweighted = acc_coarse_unweighted.cpu().numpy()\n",
    "    valid_loss_coarse = valid_loss_coarse.cpu().numpy()\n",
    "    acc_land = acc_land.cpu().numpy()\n",
    "    acc_sea = acc_sea.cpu().numpy()\n",
    "\n",
    "    return (\n",
    "        np.expand_dims(seq_real[n_history:], 0),\n",
    "        np.expand_dims(seq_pred[n_history:], 0),\n",
    "        np.expand_dims(valid_loss, 0),\n",
    "        np.expand_dims(acc, 0),\n",
    "        np.expand_dims(acc_unweighted, 0),\n",
    "        np.expand_dims(valid_loss_coarse, 0),\n",
    "        np.expand_dims(acc_coarse, 0),\n",
    "        np.expand_dims(acc_coarse_unweighted, 0),\n",
    "        np.expand_dims(acc_land, 0),\n",
    "        np.expand_dims(acc_sea, 0),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e116043-e5d1-49d2-8417-5f2415b88a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937bb83-8ff8-4a5b-9604-06c5abdadf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hours_to_datetime(hours, start_year):\n",
    "    days = hours // 24\n",
    "    hours = hours % 24\n",
    "    \n",
    "    start_date = datetime(start_year, 1, 1, 0, 0, 0)\n",
    "    date = start_date + timedelta(days= int(days), hours= int(hours))\n",
    "    return date\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb607fea-360e-4ff2-ab81-21bc7a2a8170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880712c5-696f-4876-a589-8f78e01d1429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f24ad2-4a77-4e4c-8c0d-32d551a9d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--run_num', default='00', type=str)\n",
    "    parser.add_argument('--yaml_config', default=f\"/scratch/gilbreth/{username}/FourCastNet/config/AFNO.yaml\",\n",
    "                        type=str)\n",
    "    \n",
    "    # defaul full_field vs afno_backbone\n",
    "    parser.add_argument('--config', default='full_field', type=str)\n",
    "    parser.add_argument('--use_daily_climatology', action='store_true')\n",
    "    parser.add_argument(\"--fld\", default=\"z500\", type=str )\n",
    "    \n",
    "    parser.add_argument('--vis', action='store_true')\n",
    "    parser.add_argument('--override_dir', default=None, type=str,\n",
    "                        help='Path to store inference outputs; must also set --weights arg')\n",
    "    parser.add_argument('--interp', default=0, type=float)\n",
    "    parser.add_argument('--weights', default=\"/scratch/gilbreth/gupt1075/model_weights/FCN_weights_v0/backbone.ckpt\", type=str,\n",
    "                        help='Path to model weights, for use with override_dir option')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    params = YParams(os.path.abspath(args.yaml_config), args.config)\n",
    "    \n",
    "    params[\"fld\"] = args.fld\n",
    "    params['world_size'] = 1\n",
    "    params['interp'] = args.interp\n",
    "    params['use_daily_climatology'] = args.use_daily_climatology\n",
    "    params['global_batch_size'] = params.batch_size\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    vis = args.vis\n",
    "\n",
    "\n",
    "\n",
    "    # fld = 'z500'  # diff flds have diff decor times and hence differnt ics\n",
    "    if args.fld == 'z500' or args.fld == '2m_temperature' or args.fld == 't850':\n",
    "        params[\"DECORRELATION_TIME\"] = 36  # 9 days (36) for z500, 2 (8 steps) days for u10, v10\n",
    "    else:\n",
    "        params[\"DECORRELATION_TIME\"] = 8  # 9 days (36) for z500, 2 (8 steps) days for u10, v10\n",
    "    params[\"idxes\"] = {\"u10\": 0, \"z500\": 14, \"2m_temperature\": 2, \"v10\": 1, \"t850\": 5}\n",
    "\n",
    "\n",
    "\n",
    "    # Set up directory\n",
    "\n",
    "    if args.override_dir is not None:\n",
    "        assert args.weights is not None, \\\n",
    "            'Must set --weights argument if using --override_dir'\n",
    "        expDir = args.override_dir\n",
    "    else:\n",
    "        assert args.weights is None, \\\n",
    "            'Cannot use --weights argument without also using --override_dir'\n",
    "        expDir = os.path.join(params.exp_dir, args.config,\n",
    "                              str(args.run_num))\n",
    "\n",
    "    if not os.path.isdir(expDir):\n",
    "        os.makedirs(expDir)\n",
    "\n",
    "    params['experiment_dir'] = os.path.abspath(expDir)\n",
    "    params['best_checkpoint_path'] = (args.weights if args.override_dir\n",
    "            is not None else os.path.join(expDir,\n",
    "            'training_checkpoints/best_ckpt.tar'))\n",
    "    params['resuming'] = False\n",
    "    params['local_rank'] = 0\n",
    "\n",
    "    logging_utils.log_to_file(logger_name=None,\n",
    "                              log_filename=os.path.join(expDir,\n",
    "                              'inference_out.log'))\n",
    "    logging_utils.log_versions()\n",
    "    params.log()\n",
    "\n",
    "    n_ics = params['n_initial_conditions']\n",
    "\n",
    "    if args.fld== 'z500' or args.fld == 't850':\n",
    "        n_samples_per_year = 1336\n",
    "    else:\n",
    "        n_samples_per_year = 1460\n",
    "\n",
    "    if params['ics_type'] == 'default':\n",
    "        num_samples = n_samples_per_year - params.prediction_length\n",
    "        stop = num_samples\n",
    "        ics = np.arange(0, stop, params[\"DECORRELATION_TIME\"])\n",
    "        if vis:  # visualization for just the first ic (or any ic)\n",
    "            ics = [0]\n",
    "        n_ics = len(ics)\n",
    "        logging.warning(f\" \\n ICS for default: {ics} num_samples {num_samples}  prediction_lnegth: {params.prediction_length}   \")\n",
    "        # logging.warning(\"Inference for {} initial conditions with ics_type {} : current_date {}  and hours_since_jan_01_epoch  {} \".format(n_ics, params[\"ics_type\"],  date_strings, hours_since_jan_01_epoch ))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    elif params['ics_type'] == 'datetime':\n",
    "        date_strings = params['date_strings']\n",
    "        ics = []\n",
    "        if params.perturb:  # for perturbations use a single date and create n_ics perturbations\n",
    "            n_ics = params['n_perturbations']\n",
    "            date = date_strings[0]\n",
    "            date_obj = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "            day_of_year = date_obj.timetuple().tm_yday - 1\n",
    "            hour_of_day = date_obj.timetuple().tm_hour\n",
    "            hours_since_jan_01_epoch = 24 * day_of_year + hour_of_day\n",
    "            for ii in range(n_ics):\n",
    "                ics.append(int(hours_since_jan_01_epoch / 6))\n",
    "        else:\n",
    "            for date in date_strings:\n",
    "                date_obj = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "                day_of_year = date_obj.timetuple().tm_yday - 1\n",
    "                hour_of_day = date_obj.timetuple().tm_hour\n",
    "                hours_since_jan_01_epoch = 24 * day_of_year \\\n",
    "                    + hour_of_day\n",
    "                ics.append(int(hours_since_jan_01_epoch / 6))\n",
    "        n_ics = len(ics)\n",
    "        logging.warning(f\" #### ICS for datetime: {ics} \")\n",
    "        logging.warning(\"Inference for {} initial conditions with ics_type {} : current_date {}  and hours_since_jan_01_epoch  {} \".format(n_ics, params[\"ics_type\"],  date_strings, hours_since_jan_01_epoch ))\n",
    "        logging.warning(f\"{date} {date_obj} {day_of_year} {hour_of_day} {hours_since_jan_01_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "    # logging.info('Inference for {} initial conditions'.format(n_ics))\n",
    "    # logging.warning('\\n Listed ics {} initial conditions'.format(ics))\n",
    "    \n",
    "    try:\n",
    "        autoregressive_inference_filetag = params['inference_file_tag']\n",
    "    except:\n",
    "        autoregressive_inference_filetag = ''\n",
    "\n",
    "    if params.interp > 0:\n",
    "        autoregressive_inference_filetag = '_coarse'\n",
    "\n",
    "    autoregressive_inference_filetag += '_' + args.fld + ''\n",
    "    if vis:\n",
    "        autoregressive_inference_filetag += '_vis'\n",
    "\n",
    "    # get data and models\n",
    "\n",
    "    (valid_data_full, model) = setup(params)\n",
    "\n",
    "    # initialize lists for image sequences and RMSE/ACC\n",
    "\n",
    "    valid_loss = []\n",
    "    valid_loss_coarse = []\n",
    "    acc_unweighted = []\n",
    "    acc = []\n",
    "    acc_coarse = []\n",
    "    acc_coarse_unweighted = []\n",
    "    seq_pred = []\n",
    "    seq_real = []\n",
    "    acc_land = []\n",
    "    acc_sea = []\n",
    "\n",
    "    # run autoregressive inference for multiple initial conditions\n",
    "\n",
    "    for (i, ic) in enumerate(ics):\n",
    "        logging.warning('Initial condition {} of {}'.format(i + 1, n_ics))\n",
    "        (\n",
    "            sr,\n",
    "            sp,\n",
    "            vl,\n",
    "            a,\n",
    "            au,\n",
    "            vc,\n",
    "            ac,\n",
    "            acu,\n",
    "            accland,\n",
    "            accsea,\n",
    "            ) = autoregressive_inference(params, ic, valid_data_full,\n",
    "                    model)\n",
    "\n",
    "\n",
    "        date_object = hours_to_datetime(ics[i], 2018)\n",
    "        # Format the date to get the day and month\n",
    "        date_string = date_object.strftime(\"%d_%B_%H:%M:%S\")\n",
    "        \n",
    "        with open(f\"{expDir}/seq_pred_output_{i}_datetime_{date_string}.npy\", 'wb') as f:\n",
    "            np.save(f, np.squeeze(sp))\n",
    "        with open(f\"{expDir}/seq_real_output_{i}_datetime_{date_string}.npy\", 'wb') as f:\n",
    "            np.save(f, np.squeeze(sr)) \n",
    "        logging.warning(f\" saved real and predicted with shape {sp.shape} {sr.shape} \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if i == 0 or len(valid_loss) == 0:\n",
    "            seq_real = sr\n",
    "            seq_pred = sp\n",
    "            valid_loss = vl\n",
    "            valid_loss_coarse = vc\n",
    "            acc = a\n",
    "            acc_coarse = ac\n",
    "            acc_coarse_unweighted = acu\n",
    "            acc_unweighted = au\n",
    "            acc_land = accland\n",
    "            acc_sea = accsea\n",
    "        else:\n",
    "\n",
    "#        seq_real = np.concatenate((seq_real, sr), 0)\n",
    "#        seq_pred = np.concatenate((seq_pred, sp), 0)\n",
    "\n",
    "            valid_loss = np.concatenate((valid_loss, vl), 0)\n",
    "            valid_loss_coarse = np.concatenate((valid_loss_coarse, vc),\n",
    "                    0)\n",
    "            acc = np.concatenate((acc, a), 0)\n",
    "            acc_coarse = np.concatenate((acc_coarse, ac), 0)\n",
    "            acc_coarse_unweighted = \\\n",
    "                np.concatenate((acc_coarse_unweighted, acu), 0)\n",
    "            acc_unweighted = np.concatenate((acc_unweighted, au), 0)\n",
    "            acc_land = np.concatenate((acc_land, accland), 0)\n",
    "            acc_sea = np.concatenate((acc_sea, accsea), 0)\n",
    "\n",
    "    prediction_length = seq_real[0].shape[0]\n",
    "    n_out_channels = seq_real[0].shape[1]\n",
    "    img_shape_x = seq_real[0].shape[2]\n",
    "    img_shape_y = seq_real[0].shape[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ec116-7ef1-400d-8741-92803e6e8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save predictions and loss\n",
    "\n",
    "    if params.log_to_screen:\n",
    "        logging.info('Saving files at {}'.format(os.path.join(params['experiment_dir'\n",
    "                     ], 'autoregressive_predictions'\n",
    "                     + autoregressive_inference_filetag + '.h5')))\n",
    "    with h5py.File(os.path.join(params['experiment_dir'],\n",
    "                   'autoregressive_predictions'\n",
    "                   + autoregressive_inference_filetag + '.h5'), 'a') as \\\n",
    "        f:\n",
    "        if vis:\n",
    "            try:\n",
    "                f.create_dataset('ground_truth', data=seq_real,\n",
    "                                 shape=(n_ics, prediction_length,\n",
    "                                 n_out_channels, img_shape_x,\n",
    "                                 img_shape_y), dtype=np.float32)\n",
    "            except:\n",
    "                del f['ground_truth']\n",
    "                f.create_dataset('ground_truth', data=seq_real,\n",
    "                                 shape=(n_ics, prediction_length,\n",
    "                                 n_out_channels, img_shape_x,\n",
    "                                 img_shape_y), dtype=np.float32)\n",
    "                f['ground_truth'][...] = seq_real\n",
    "\n",
    "            try:\n",
    "                f.create_dataset('predicted', data=seq_pred,\n",
    "                                 shape=(n_ics, prediction_length,\n",
    "                                 n_out_channels, img_shape_x,\n",
    "                                 img_shape_y), dtype=np.float32)\n",
    "            except:\n",
    "                del f['predicted']\n",
    "                f.create_dataset('predicted', data=seq_pred,\n",
    "                                 shape=(n_ics, prediction_length,\n",
    "                                 n_out_channels, img_shape_x,\n",
    "                                 img_shape_y), dtype=np.float32)\n",
    "                f['predicted'][...] = seq_pred\n",
    "\n",
    "        if params.masked_acc:\n",
    "            try:\n",
    "                f.create_dataset('acc_land', data=acc_land)  # , shape = (n_ics, prediction_length, n_out_channels), dtype =np.float32)\n",
    "            except:\n",
    "                del f['acc_land']\n",
    "                f.create_dataset('acc_land', data=acc_land)  # , shape = (n_ics, prediction_length, n_out_channels), dtype =np.float32)\n",
    "                f['acc_land'][...] = acc_land\n",
    "\n",
    "            try:\n",
    "                f.create_dataset('acc_sea', data=acc_sea)  # , shape = (n_ics, prediction_length, n_out_channels), dtype =np.float32)\n",
    "            except:\n",
    "                del f['acc_sea']\n",
    "                f.create_dataset('acc_sea', data=acc_sea)  # , shape = (n_ics, prediction_length, n_out_channels), dtype =np.float32)\n",
    "                f['acc_sea'][...] = acc_sea\n",
    "\n",
    "        try:\n",
    "            f.create_dataset('rmse', data=valid_loss, shape=(n_ics,\n",
    "                             prediction_length, n_out_channels),\n",
    "                             dtype=np.float32)\n",
    "        except:\n",
    "            del f['rmse']\n",
    "            f.create_dataset('rmse', data=valid_loss, shape=(n_ics,\n",
    "                             prediction_length, n_out_channels),\n",
    "                             dtype=np.float32)\n",
    "            f['rmse'][...] = valid_loss\n",
    "\n",
    "        try:\n",
    "            f.create_dataset('acc', data=acc, shape=(n_ics,\n",
    "                             prediction_length, n_out_channels),\n",
    "                             dtype=np.float32)\n",
    "        except:\n",
    "            del f['acc']\n",
    "            f.create_dataset('acc', data=acc, shape=(n_ics,\n",
    "                             prediction_length, n_out_channels),\n",
    "                             dtype=np.float32)\n",
    "            f['acc'][...] = acc\n",
    "\n",
    "        try:\n",
    "            f.create_dataset('rmse_coarse', data=valid_loss_coarse,\n",
    "                             shape=(n_ics, prediction_length,\n",
    "                             n_out_channels), dtype=np.float32)\n",
    "        except:\n",
    "            del f['rmse_coarse']\n",
    "            f.create_dataset('rmse_coarse', data=valid_loss_coarse,\n",
    "                             shape=(n_ics, prediction_length,\n",
    "                             n_out_channels), dtype=np.float32)\n",
    "            f['rmse_coarse'][...] = valid_loss_coarse\n",
    "\n",
    "        try:\n",
    "            f.create_dataset('acc_coarse', data=acc_coarse,\n",
    "                             shape=(n_ics, prediction_length,\n",
    "                             n_out_channels), dtype=np.float32)\n",
    "        except:\n",
    "            del f['acc_coarse']\n",
    "            f.create_dataset('acc_coarse', data=acc_coarse,\n",
    "                             shape=(n_ics, prediction_length,\n",
    "                             n_out_channels), dtype=np.float32)\n",
    "            f['acc_coarse'][...] = acc_coarse\n",
    "\n",
    "        try:\n",
    "            f.create_dataset('acc_unweighted', data=acc_unweighted,\n",
    "                             shape=(n_ics, prediction_length,\n",
    "                             n_out_channels), dtype=np.float32)\n",
    "        except:\n",
    "            del f['acc_unweighted']\n",
    "            f.create_dataset('acc_unweighted', data=acc_unweighted,\n",
    "                             shape=(n_ics, prediction_length,\n",
    "                             n_out_channels), dtype=np.float32)\n",
    "            f['acc_unweighted'][...] = acc_unweighted\n",
    "\n",
    "        try:\n",
    "            f.create_dataset('acc_coarse_unweighted',\n",
    "                             data=acc_coarse_unweighted, shape=(n_ics,\n",
    "                             prediction_length, n_out_channels),\n",
    "                             dtype=np.float32)\n",
    "        except:\n",
    "            del f['acc_coarse_unweighted']\n",
    "            f.create_dataset('acc_coarse_unweighted',\n",
    "                             data=acc_coarse_unweighted, shape=(n_ics,\n",
    "                             prediction_length, n_out_channels),\n",
    "                             dtype=np.float32)\n",
    "            f['acc_coarse_unweighted'][...] = acc_coarse_unweighted\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872629d9-2b92-41e2-a5a8-45915cac327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# BSD 3-Clause License\n",
    "#\n",
    "# Copyright (c) 2022, FourCastNet authors\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "#\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "#\n",
    "# 3. Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# The code was authored by the following people:\n",
    "#\n",
    "# Jaideep Pathak - NVIDIA Corporation\n",
    "# Shashank Subramanian - NERSC, Lawrence Berkeley National Laboratory\n",
    "# Peter Harrington - NERSC, Lawrence Berkeley National Laboratory\n",
    "# Sanjeev Raja - NERSC, Lawrence Berkeley National Laboratory\n",
    "# Ashesh Chattopadhyay - Rice University\n",
    "# Morteza Mardani - NVIDIA Corporation\n",
    "# Thorsten Kurth - NVIDIA Corporation\n",
    "# David Hall - NVIDIA Corporation\n",
    "# Zongyi Li - California Institute of Technology, NVIDIA Corporation\n",
    "# Kamyar Azizzadenesheli - Purdue University\n",
    "# Pedram Hassanzadeh - Rice University\n",
    "# Karthik Kashinath - NVIDIA Corporation\n",
    "# Animashree Anandkumar - California Institute of Technology, NVIDIA Corporation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirain_pytorch",
   "language": "python",
   "name": "chirain_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
